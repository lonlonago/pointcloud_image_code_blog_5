#  The Background of Point Cloud Semantic Segmentation 

 Point cloud semantic segmentation has great application value in 3D object detection, scene recognition and high-precision map automation construction. With the wide application of deep neural networks in the field of computer vision, point cloud semantic segmentation methods based on deep learning have become the mainstream in this field. Although lidar has high ranging accuracy, there are also several problems: 

 How to extract context information from irregularly distributed points in space, whether local or global. From the perspective of methods for aggregating context information, there are two main ways: parameterization and non-parameterization 

 In order to learn the characteristics of point clouds and solve the problem of point cloud disorder: 

 Extracting the two-dimensional feature map allows machine learning using typical image semantic segmentation neural networks, which originated from FCNs. The basic feature is encoding and decoding, that is, the two-dimensional data features are first compressed to establish high-level features, and then enlarged to learn local features. More advanced methods include Deeplab and UNet. Both aim to fuse contextual information at multiple scales. DeepLab and its successors use diluted convolutional filters to increase the receiving field, while Unet adds skip connections to directly connect different levels of semantic features, which are more effective in images with irregular and rough edges, such as medical images 

 There are currently many LiDAR object detection datasets, such as the Waymo Open Dataset and the KITTI 3D detection dataset. In contrast, LiDAR scan semantic segmentation datasets are relatively rare. There are three commonly used semantic segmentation datasets: Audi dataset, Paris-Lille-3D, and Semantic KITTI dataset. 

#  Conclusion 

 The contributions of PolarNet's work are as follows: 

#  PolarNet Approach 

>  Figure 2. An overview of our model. For a given LiDAR point cloud, we first quantify the points into a grid using polar coordinate BEV. For each grid cell, we use a simplified KNN-free PointNet to transform the points in it into a fixed-length representation. This representation is then assigned to its corresponding position in the ring matrix. We feed the matrix into a ring CNN composed of ring convolutional modules. Finally, the CNN outputs a quantized prediction, which we decode into the point domain 

>   

##  Problem Statement 

 The training dataset for a given N-frame lidar scan is the i-th frame scan containing LiDAR points, containing four dimensional information (x, y, z, reflection) 

##  Polar Birdâ€™s-eye-view 

 ![avatar]( 8335bb59ac0848cdae01c7b95d6ed89d.png) 

>  Figure 4. The relationship between the distance from the grid cells of the sensor and the average number of points in the logarithmic interval of each grid cell. A traditional BEV represents distributing most of its grid cells to a farther end with only a few points. 

 The mesh is the basic image representation, but it may not be the best representation of BEV, which is a compromise between performance and accuracy. 

##  Learning the Polar Grid 

 Unlike cnn-Seg, which uses manual features, PolarNet uses a fixed-length representation to capture the distribution of points in each grid. It is generated by a learnable simplified PointNet and a max-pooling. The characteristics of the first grid cell in a scan ring are: 

 ![avatar]( c1c6a58c7a7648909db1e21052637e1a.png) 

 Where w and l are the quantized size. And is the position of the point p in the map. Note that the position and quantized size can be polar or Cartesian coordinates. We do not quantize the input point cloud along the z-axis, our method represents the entire vertical column of the grid 

 If the representation is learned in polar coordinates, then the two sides of the feature matrix will be connected along the azimuth axis in physical space, developing a discrete convolution which we call ring convolution. Assuming that the matrix is connected at both ends of the radius axis, the ring convolution kernel will convolution the matrix. At the same time, the layer located on the opposite side can propagate back to the opposite side through this ring convolution kernel. By replacing ordinary convolution with ring convolution in a 2D network, the network will be able to handle the polar grid end-to-end without neglecting its connectivity. This provides the model with an extended receptive field. Since it is a 2D neural network, the final prediction will also be a polar grid with feature dimensions equal to the product of quantized height channels and class numbers. We can then reshape the prediction into a 4D matrix to derive the segmentation loss based on voxels. 

